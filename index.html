<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description"
    content="A Hitchhikers Guide to Fine-Grained Face Forgery Detection Using Common Sense Reasoning">
  <meta name="keywords" content="deepfake detection, benchmark, VLLM">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>A Hitchhiker's Guide to Fine-Grained Face Forgery Detection Using Common Sense Reasoning</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
<!--   <link rel="icon" href="./static/images/favicon.svg"> -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>

  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
        <a class="navbar-item" href="https://www.uni.lu/snt-en/research-groups/cvi2/">
          <span class="icon">
            <i class="fas fa-home"></i>
          </span>
        </a>

<!--         <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link">
            Additional Resources
          </a>
          <div class="navbar-dropdown">
            
              <a class="navbar-item" href="https://www.project1.com">
                Project One
              </a>
            
              <a class="navbar-item" href="https://www.project2.com">
                Project Two
              </a>
            
          </div>
        </div> -->
      </div>
    </div>
  </nav>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">A Hitchhiker's Guide to Fine-Grained Face Forgery Detection Using Common Sense Reasoning</h1>
            <div class="is-size-5 publication-authors">
              
                <span class="author-block">
                  <a href="https://nickyfot.github.io">Niki Foteinopoulou</a><sup>['1']</sup>
                </span>
              
                <span class="author-block">
                  <a href="">Enjie Ghorbel</a><sup>['1', '2']</sup>
                </span>
              
                <span class="author-block">
                  <a href="">Djamila Aouada</a><sup>['1']</sup>
                </span>
              
            </div>

            <div class="is-size-5 publication-authors">
              
                <span class="author-block"><sup>1</sup>CVI2, SnT, University of Luxembourg</span>
              
                <span class="author-block"><sup>2</sup>Cristal Laboratory, National School of Computer Sciences, University of Manouba</span>
              
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                
                  <span class="link-block">
                    <a href="https://arxiv.org/pdf/2410.00485" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Paper</span>
                    </a>
                  </span>
                
                  <span class="link-block">
                    <a href="https://arxiv.org/abs/2410.00485" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="ai ai-arxiv"></i>
                      </span>
                      <span>arXiv</span>
                    </a>
                  </span>
                
                  <span class="link-block">
                    <a href="https://github.com/NickyFot/HitchhikersGuide" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                    </a>
                  </span>
                
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <video id="teaser" autoplay muted loop playsinline height="100%">
          <source src="./static/videos/teaser.mp4" type="video/mp4">
        </video>
        <h2 class="subtitle has-text-centered">
          <span class="dnerf">Nerfies</span> turns selfie videos from your phone into
          free-viewpoint
          portraits.
        </h2>
      </div>
    </div>
  </section> -->


  <!-- <section class="hero is-light is-small">
    <div class="hero-body">
      <div class="container">
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item item-steve">
            <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/steve.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-chair-tp">
            <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/chair-tp.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-shiba">
            <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/shiba.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-fullbody">
            <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/fullbody.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-blueshirt">
            <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/blueshirt.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-mask">
            <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/mask.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-coffee">
            <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/coffee.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-toby">
            <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/toby2.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
  </section> -->



  
  <section class="section">
    <div class="container is-max-desktop">
      <!-- Section. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="block content has-text-justified">
            
              <p>Explainability in artificial intelligence is crucial for restoring trust, particularly in areas like face forgery detection, where viewers often struggle to distinguish between real and fabricated content. Vision and Large Language Models (VLLM) bridge computer vision and natural language, offering numerous applications driven by strong common-sense reasoning. Despite their success in various tasks, the potential of vision and language remains underexplored in face forgery detection, where they hold promise for enhancing explainability by leveraging the intrinsic reasoning capabilities of language to analyse fine-grained manipulation areas. As such, there is a need for a methodology that converts face forgery detection to a Visual Question Answering (VQA) task to systematically and fairly evaluate these capabilities. Previous efforts for unified benchmarks in deepfake detection have focused on the simpler binary task, overlooking evaluation protocols for fine-grained detection and text-generative models. We propose a multi-staged approach that diverges from the traditional binary decision paradigm to address this gap. In the first stage, we assess the models' performance on the binary task and their sensitivity to given instructions using several prompts. In the second stage, we delve deeper into fine-grained detection by identifying areas of manipulation in a multiple-choice VQA setting. In the third stage, we convert the fine-grained detection to an open-ended question and compare several matching strategies for the multi-label classification task. Finally, we qualitatively evaluate the fine-grained responses of the VLLMs included in the benchmark. We apply our benchmark to several popular models, providing a detailed comparison of binary, multiple-choice, and open-ended VQA evaluation across seven datasets.</p>
<!--               <div class="block center-media">
                
                  
                    
                      <img src="./static/images/interpolate_end.jpg" alt="Embedded image">
                    
                  
                
              </div> -->
            
            
              <div class="block center-media">
                
                  <iframe src="https://www.youtube.com/embed/abstract_video" allowfullscreen></iframe>
                
              </div>
            
          </div>
          
        </div>
      </div>
      <!--/ Section. -->

      <!-- Paper video. -->
      <!-- <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Video</h2>
          <div class="publication-video">
            <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0" frameborder="0"
              allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div> -->
      <!--/ Paper video. -->
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Section. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Results</h2>
          <div class="block content has-text-justified">
            
            
<!--               <div class="block center-media">
                
                  
                    <img src="./static/images/interpolate_start.jpg">
                  
                
              </div> -->
            
          </div>
          
            <div class="subsections block content has-text-justified">
              
                <h3 class="title is-4">Binary Classification</h3>
                <div class="block content has-text-justified">
                  
                  
                    <p>Robustness to different prompts: We use seven synonyms for the positive class: ``manipulated'', ``deepfake'', ``synthetic'', ``altered'', ``fabricated'', ``face forgery''  and ``falsified''. As the binary task is simple and the instruction format is a `Yes' or `No' question, we use EM as a matching approach in this evaluation. We see the performance of each tested model under the binary detection setting on the two sub-sets of SeqDeepFake and the R-splicer dataset using the three best performing synonyms: ``manipulated'', ``synthetic'' and ``altered''. The first observation is that no VLLM clearly outperforms others across all datasets and metrics. However, we see that BLIP-2 has the most robust performance to the given instruction, even though it is the smallest in terms of parameters. Furthermore,  the additional parameters of T5-xxl do not seem to aid the task compared to the base InstructBLIP with T5 generator, as the base model performs comparably better across most benchmarks. We theorise that as the VLLMs have not been explicitly trained on image-language pairs of manipulated images, a large number of parameters on the language generation leads to more hallucinations for this simple but abstract task. Compared to the CLIP, models appear to have competitive performance with the exception of InstructBLIP with T5xxl LLM. When both base models, i.e. BLIP-2 and LlaVa, have relatively good performance, the ensemble shows marginal improvement, particularly in terms of Accuracy and F1; however, this is not consistent therefore we do not continue the investigation to fine-grained labels. The detailed performance of all models and synonyms across all datasets and additional analysis on CLIP features can be found in the Appendix.</p>
<!--                     <div class="block center-media">
                      
                        
                          
                            <img src="./static/images/table_1.jpg" alt="Embedded image">
                          
                        
                      
                    </div> -->
                  
                    <p>Overall model performance: We average the performance of the three best-performing synonyms on all nine benchmarks in [?]. No model clearly outperforms others across all metrics and datasets; however, we can observe competitive performance from BLIP-2 on the binary task, even though it is the smallest model in terms of parameters. We also see that all models struggle with the more challenging in-the-wild datasets, such as CelebDF, which highlights the need for further development to achieve adequate generalisation. Performance of GPT4v should be treated as an upper bound as we cannot assess whether the model has been trained on samples of the selected datasets. We see that GPT4v vastly outperforms the selected VLLMs on three benchmarks and has comparable performance on the rest, with the exception of FF++.</p>
                    <div class="block center-media">
                      
                    </div>
                  
                    <p>Vision Encoder Finetuning: We finetune contrastively the vision encoder of LlaVa on FF++ using a sigmoid loss over an ensemble of prompts for the real/fake categories, and evaluate as described in the previous section. Training details for the vision encoder can be found in [?]. The architecture with the fine-tuned vision encoder shows improved within dataset and cross-dataset performance as shown in [?]. Even without detailed captions or updating the LLM weights, we see there are still gains from a task specific vision encoder, particularly in terms of F1-score with an average improvement of nearly 4% within dataset and nearly 2% cross-dataset (for SeqDeepFake, CelebDF and StyleGAN2).</p>
                    <div class="block center-media">
                      
                    </div>
                  
                    <p>Metrics: In terms of the selected metrics, following the initial intuition, there is limited information we can get from the standard Accuracy and AUC used in the binary task. Both are heavily skewed by the label distribution, which is typically imbalanced in deepfake datasets; however, the latter may also not be fit for VLLMs as AUC measures performance at different thresholds, which are not present with EM and contains matching strategies. As such, we argue that for the task at hand, the F1-score --and consequently robust to imbalance metrics-- are more appropriate.</p>
                    <div class="block center-media">
                      
                    </div>
                  
                </div>
              
                <h3 class="title is-4">Finegrained Evaluation</h3>
                <div class="block content has-text-justified">
                  
                  
                    <p>For the fine-grained task, we evaluate the performance of the selected models in the open and closed vocabulary settings. The fine-grained labels are evaluated on samples where the ground truth is positive -- i.e., on DeepFake samples.</p>
                    <div class="block center-media">
                      
                        
                          
                            <img src="./static/images/table_1.jpg" alt="Embedded image">
                          
                        
                      
                    </div>
                  
                    <p>Open-Ended VQA: We first evaluate the selected VLLMs under the open vocabulary VQA setting on the three fine-grained datasets. The results using contains and CLIP distance matching are reported in [?]  and [?] respectively. An EM strategy is not possible in multi-label tasks, so no such evaluation is performed. No model clearly outperforms others across all metrics and datasets. In fact, we can observe that, in most cases, they have comparable performance. This holds true for both contains and CLIP distance metrics. In terms of matching strategy, using the CLIP distance consistently and greatly improves recall, as is evident by the improvement in the F1-score and explicitly shown in [?]. This matching approach slightly lowers the mAP and AUC scores compared to the contains metrics; however, using the cosine distance to match the open-ended responses to the class categories semantically may offer a more reliable output for the class of interest, as seen by the F1-score.</p>
                    <div class="block center-media">
                      
                    </div>
                  
                    <p>Multiple choice VQA: The performance of the VLLMs on the multiple-choice instruction is shown in [?]. Even though the open-ended setting is theoretically more challenging, the performances of all tested models are comparable to each other and worse on the multiple-choice instruction for both mAP and AUC. Regarding the F1-score, however, LlaVa consistently performs better than other models. Under the multiple-choice setting, we observe that the models tend to mention all label names, which raises the number of False Positives --a significant limitation of the multiple choice setting-- or respond with ``All of them'' or ``None of them'', which makes matching of any sort more challenging and is reflected even more in the lower F1 score. includes detailed metrics for each category. </p>
                    <div class="block center-media">
                      
                    </div>
                  
                </div>
              
            </div>
          
        </div>
      </div>
      <!--/ Section. -->

      <!-- Paper video. -->
      <!-- <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Video</h2>
          <div class="publication-video">
            <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0" frameborder="0"
              allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div> -->
      <!--/ Paper video. -->
    </div>
  </section>

        

  <section class="section">
    <div class="container is-max-desktop">

      <!-- <div class="columns is-centered"> -->

        <!-- Visual Effects. -->
        <!-- <div class="column">
          <div class="content">
            <h2 class="title is-3">Visual Effects</h2>
            <p>
              Using <i>nerfies</i> you can create fun visual effects. This Dolly zoom effect
              would be impossible without nerfies since it would require going through a wall.
            </p>
            <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/dollyzoom-stacked.mp4" type="video/mp4">
            </video>
          </div>
        </div> -->
        <!--/ Visual Effects. -->

        <!-- Matting. -->
        <!-- <div class="column">
          <h2 class="title is-3">Matting</h2>
          <div class="columns is-centered">
            <div class="column content">
              <p>
                As a byproduct of our method, we can also solve the matting problem by ignoring
                samples that fall outside of a bounding box during rendering.
              </p>
              <video id="matting-video" controls playsinline height="100%">
                <source src="./static/videos/matting.mp4" type="video/mp4">
              </video>
            </div>

          </div>
        </div>
      </div> -->
      <!--/ Matting. -->

      <!-- Animation. -->
      <!-- <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Animation</h2> -->

          <!-- Interpolating. -->
          <!-- <h3 class="title is-4">Interpolating states</h3>
          <div class="content has-text-justified">
            <p>
              We can also animate the scene by interpolating the deformation latent codes of two input
              frames. Use the slider here to linearly interpolate between the left frame and the right
              frame.
            </p>
          </div>
          <div class="columns is-vcentered interpolation-panel">
            <div class="column is-3 has-text-centered">
              <img src="./static/images/interpolate_start.jpg" class="interpolation-image"
                alt="Interpolate start reference image." />
              <p>Start Frame</p>
            </div>
            <div class="column interpolation-video-column">
              <div id="interpolation-image-wrapper">
                Loading...
              </div>
              <input class="slider is-fullwidth is-large is-info" id="interpolation-slider" step="1" min="0" max="100"
                value="0" type="range">
            </div>
            <div class="column is-3 has-text-centered">
              <img src="./static/images/interpolate_end.jpg" class="interpolation-image"
                alt="Interpolation end reference image." />
              <p class="is-bold">End Frame</p>
            </div>
          </div>
          <br /> -->
          <!--/ Interpolating. -->

          <!-- Re-rendering. -->
          <!-- <h3 class="title is-4">Re-rendering the input video</h3>
          <div class="content has-text-justified">
            <p>
              Using <span class="dnerf">Nerfies</span>, you can re-render a video from a novel
              viewpoint such as a stabilized camera by playing back the training deformations.
            </p>
          </div>
          <div class="content has-text-centered">
            <video id="replay-video" controls muted preload playsinline width="75%">
              <source src="./static/videos/replay.mp4" type="video/mp4">
            </video>
          </div> -->
          <!--/ Re-rendering. -->

        <!-- </div>
      </div> -->
      <!--/ Animation. -->


      <!-- Concurrent Work. -->
<!--      <div class="columns is-centered">-->
<!--        <div class="column is-full-width">-->
<!--          <h2 class="title is-3">Related Works</h2>-->

<!--          <div class="content has-text-justified">-->
<!--            -->
<!--            <p>-->
<!--              -->
<!--                <a href="https://www.relatedwork1.com">Related Work 1</a>-->
<!--              -->
<!--                <a href="https://www.relatedwork2.com">Related Work 2</a>-->
<!--              -->
<!--              This is a body text about the first related work.-->
<!--            </p>-->
<!--            -->
<!--            <p>-->
<!--              -->
<!--                <a href="https://www.relatedwork3.com">Related Work 3</a>-->
<!--              -->
<!--                <a href="https://www.relatedwork4.com">Related Work 4</a>-->
<!--              -->
<!--              This is a body text about the second related work.-->
<!--            </p>-->
<!--            -->
<!--          </div>-->
<!--        </div>-->
<!--      </div>-->
      <!--/ Concurrent Work. -->

    </div>
  </section>


  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@inproceedings{
      foteinopoulou2024hitchhikers,
      title={A Hitchhikers Guide to Fine-Grained Face Forgery Detection Using Common Sense Reasoning},
      author={Anonymous},
      booktitle={The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
      year={2024},
      url={https://openreview.net/forum?id=cR3T1ZYN8I}
      }</code></pre>
    </div>
  </section>


  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        
          <a class="icon-link" href="https://github.com/nickyfot">
            <i class="fab fa-github"></i>
          </a>
        
      </div>
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
            <p>
              This page is created using the source code of the <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> website. Leave the link to the original source code should you decide to reuse it.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>
